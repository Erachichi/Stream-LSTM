{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\anaconda3\\envs\\python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "from plotly import graph_objects as go\n",
    "import math\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(data, sw_width, in_start=0):\n",
    "    X = []\n",
    "    data = np.array(data, type(float))\n",
    "    for _ in range(data.shape[0]):\n",
    "        in_end = in_start + sw_width\n",
    "        if (in_end>data.shape[0]):\n",
    "            break\n",
    "        else:\n",
    "            train_seq = data[in_start:in_end, :]\n",
    "            X.append(train_seq)\n",
    "            in_start += 1\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann_net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ann_net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
    "        super(lstm_net, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,  \n",
    "            hidden_size=hidden_dim,  \n",
    "            num_layers=layers, \n",
    "            bias=True,  \n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ELU(inplace=True),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(hidden_dim//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        r_out,(h_t,c_t) = self.lstm(x)\n",
    "        batch, time_step, hidden_size = r_out.shape\n",
    "        out = r_out[:, -1, :].view(batch, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_set(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        data = (self.x[id],\n",
    "                self.y[id])\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parameters\n",
    "    TIME_STEP = 31\n",
    "    BATCH_SIZE = 32 # 128\n",
    "    epoches = 200\n",
    "    \n",
    "    seed = random.randint(0, 4294967295)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    data = pd.read_excel(r'./data.xlsx')\n",
    "\n",
    "    data = np.array(data)\n",
    "    data = data[:,3:]\n",
    "    train_set = data[:3652,:]\n",
    "    test_set = data[3652:,:]\n",
    "\n",
    "    train_x = sliding_window((train_set[:, 0:-1]), TIME_STEP).astype(np.float32)\n",
    "    train_y = sliding_window((train_set[:, -1:]), TIME_STEP)[:, -1, :].astype(np.float32)\n",
    "\n",
    "    train_x = train_x.reshape(train_x.shape[0],-1)\n",
    "    train_y = train_y.squeeze()\n",
    "\n",
    "    test_x = sliding_window((test_set[:, 0:-1]), TIME_STEP).astype(np.float32)\n",
    "    test_y = sliding_window((test_set[:, -1:]), TIME_STEP)[:, -1, :].astype(np.float32)\n",
    "\n",
    "    test_x = test_x.reshape(test_x.shape[0],-1)\n",
    "    test_y = test_y.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    " # lstm\n",
    "    loader_lstm = DataLoader(Data_set(train_x, train_y), batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "    \n",
    "    model_lstm = lstm_net(input_dim=8, hidden_dim=64, output_dim=1, layers=3)\n",
    "    optimizer = optim.Adam(model_lstm.parameters(),lr = 3e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model_lstm.train()\n",
    "    lowest_loss = 1000000\n",
    "    for epoch in range(epoches):\n",
    "        for data in loader_lstm:\n",
    "            x, y = data\n",
    "            # print(x.shape)\n",
    "            # print(y.shape)\n",
    "            x, y = torch.tensor(x), torch.tensor(y)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model_lstm.forward(x).squeeze()\n",
    "            loss = criterion(pred, y.squeeze())\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "    \n",
    "        checkpoint = {\"model_state_dict\": model_lstm.state_dict(),\n",
    "                  \"optimizer_state_dic\": optimizer.state_dict(),\n",
    "                  \"loss\": loss,\n",
    "                  \"epoch\": epoch}\n",
    "        if loss.item() < lowest_loss:\n",
    "            lowest_loss = loss.item()\n",
    "            save_model_path = r'./compare_model/lstm'\n",
    "            # if os.path.exists(save_model_path):\n",
    "            #     shutil.rmtree(save_model_path)\n",
    "            # os.makedirs(save_model_path)\n",
    "            path_checkpoint = save_model_path+'/'+'lstm_epoch_{}.pkl'.format(epoch)\n",
    "            # best_checkpoint = save_model_path+'/'+'lstm_77-92_epoch_{}.pkl'.format(epoch)\n",
    "\n",
    "            torch.save(checkpoint, path_checkpoint)\n",
    "\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch), 'loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    " # ann\n",
    "    train_x = train_x.reshape(train_x.shape[0],-1)\n",
    "    loader_ann = DataLoader(Data_set(torch.tensor(train_x), torch.tensor(train_y)), batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "    \n",
    "    model_ann = ann_net(input_dim=8*TIME_STEP, hidden_dim=64, output_dim=1)\n",
    "    optimizer = optim.Adam(model_ann.parameters(),lr = 3e-2)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model_ann.train()\n",
    "    lowest_loss = 1000000\n",
    "    for epoch in range(epoches):\n",
    "        for data in loader_ann:\n",
    "            x, y = data\n",
    "            # x, y = torch.tensor(x), torch.tensor(y)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model_ann.forward(x).squeeze()\n",
    "            loss = criterion(pred, y.squeeze())\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "    \n",
    "        checkpoint = {\"model_state_dict\": model_ann.state_dict(),\n",
    "                  \"optimizer_state_dic\": optimizer.state_dict(),\n",
    "                  \"loss\": loss,\n",
    "                  \"epoch\": epoch}\n",
    "        if loss.item() < lowest_loss:\n",
    "            lowest_loss = loss.item()\n",
    "            save_model_path = r'.\\compare_model\\ann'\n",
    "            # if os.path.exists(save_model_path):\n",
    "            #     shutil.rmtree(save_model_path)\n",
    "            # os.makedirs(save_model_path)\n",
    "            path_checkpoint = save_model_path+'/'+'ann_epoch_{}.pkl'.format(epoch)\n",
    "            torch.save(checkpoint, path_checkpoint)\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch), 'loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 335)\n",
      "(4, 3, 335)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    # parameters\n",
    "    TIME_STEP = 31\n",
    "    BATCH_SIZE = 32 # 128\n",
    "    epoches = 200\n",
    "    \n",
    "    #设置随机种子\n",
    "    seed = random.randint(0, 4294967295)\n",
    "    # seed = 166143082\n",
    "    # print('随机种子：',seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "    model_num = 5\n",
    "    prediction = np.zeros((model_num,3,test_y.shape[0]))\n",
    "\n",
    "    for seed_num in range(0,model_num):\n",
    "        seed_temp = random.randint(0, 4294967295)\n",
    "        # xgboost\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.01,\n",
    "            'n_estimators': 200,\n",
    "            'seed': seed_temp\n",
    "        }\n",
    "        model_xgboost = xgb.XGBRegressor(**params)\n",
    "        model_xgboost.fit(train_x, train_y)\n",
    "        # rf\n",
    "        model_rf = RandomForestRegressor(random_state=seed_temp) \n",
    "        model_rf.fit(train_x, train_y) \n",
    "        model_svr = SVR(kernel='linear')\n",
    "        model_svr.fit(train_x, train_y)\n",
    "\n",
    "        pred_xgboost = model_xgboost.predict(test_x)\n",
    "        pred_rf = model_rf.predict(test_x)\n",
    "        pred_svr = model_svr.predict(test_x)\n",
    "\n",
    "        pred_xgboost = pred_xgboost.reshape(-1,1)\n",
    "        pred_rf = pred_rf.reshape(-1,1)\n",
    "        pred_svr = pred_svr.reshape(-1,1)\n",
    "        prediction[seed_num,0:] = pred_xgboost.squeeze()\n",
    "        prediction[seed_num,1,:] = pred_rf.squeeze()\n",
    "        prediction[seed_num,2,:] = pred_svr.squeeze()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    model_lstm_test = lstm_net(input_dim=8, hidden_dim=64, output_dim=1, layers=3)\n",
    "    model_lstm_test.load_state_dict(torch.load(r'./compare/lstm_epoch.pkl')['model_state_dict'])\n",
    "    model_lstm_test.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_lstm = model_lstm_test.forward(torch.tensor(test_x)).squeeze()\n",
    "    \n",
    "    test_x = test_x.reshape(test_x.shape[0],-1)\n",
    "    model_ann_test = ann_net(input_dim=8*TIME_STEP, hidden_dim=64, output_dim=1)\n",
    "    model_ann_test.load_state_dict(torch.load(r'./compare/ann_epoch.pkl')['model_state_dict'])\n",
    "    model_ann_test.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_ann = model_ann_test.forward(torch.tensor(test_x)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
